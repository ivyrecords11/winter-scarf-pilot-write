{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# %%"
      ],
      "metadata": {
        "id": "p5NlmZ3cFl06"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title === 0) Install ===\n",
        "# - MuJoCo official python api, Ray RLlib, Gymnasium, SpikingJelly\n",
        "#   참고: MjModel.from_xml_string / XML integrator 옵션 / 마찰계수(접선/비틀림/굴림) / Gymnasium 연동\n",
        "#   Docs: mujoco.readthedocs.io, Ray RLlib rllib-env & examples, SpikingJelly encoding\n",
        "!pip -q install \"mujoco>=3.1\" \"ray[rllib]>=2.7\" \"gymnasium>=0.29\" \"spikingjelly>=0.0.0.0.14\" --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8VFp6TWFnuR",
        "outputId": "e118192c-4821-47b3-8901-0bfc573de282"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 새 셀에서 실행\n",
        "!pip -q uninstall -y ray || true\n",
        "# 최신 안정판(예: 2.49.x 계열)으로 고정 설치 – RLlib 포함\n",
        "!pip -q install -U \"ray[rllib]==2.49.2\"\n",
        "\n",
        "# 재시작 없이 바로 import하면 꼬일 수 있으니, 세션 강제 재시작\n",
        "import os, signal, time\n",
        "print(\"Restarting runtime to finalize Ray install ...\")\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "jLHusid9HQVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gputil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gok8n5PQKpLo",
        "outputId": "b8fef44c-ef36-4003-e25d-916c502830d1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=4f7381a66fd6583b312cb3a1f458a3e6da952366a921a7b80b82d8737ff0d849\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/a8/b7/d8a067c31a74de9ca252bbe53dea5f896faabd25d55f541037\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMc2kiaNFdut",
        "outputId": "e859762f-d2d4-4121-dade-3b0f3d14df7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model OK. Sites: 100 Max delay steps: 60\n",
            "Random rollout OK. step: 50 obs_dim: (108,)\n",
            "Poisson spikes shape: (5, 108)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-22 02:35:35,272\tINFO worker.py:1951 -- Started a local Ray instance.\n",
            "2025-09-22 02:35:44,230\tWARNING deprecation.py:50 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
            "2025-09-22 02:35:44,236\tWARNING algorithm_config.py:5045 -- You are running DQN on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
            "2025-09-22 02:35:44,849\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'len_mean': None, 'ret_mean': None}\n",
            "DQN one-iter result keys: {}\n"
          ]
        }
      ],
      "source": [
        "#@title === 1) Imports & Utils ===\n",
        "import math, textwrap, numpy as np\n",
        "import mujoco as mj\n",
        "from gymnasium import Env, spaces\n",
        "from dataclasses import dataclass\n",
        "import ray\n",
        "from ray.rllib.algorithms.dqn import DQNConfig\n",
        "from ray.rllib.env.env_context import EnvContext\n",
        "\n",
        "# (Optional) SpikingJelly: 간단 레이트→포아송 인코더 예시용\n",
        "try:\n",
        "    from spikingjelly.activation_based import functional as sjF\n",
        "except Exception:\n",
        "    sjF = None\n",
        "\n",
        "DT = 1e-4   # 0.1 ms\n",
        "V  = 50.0   # m/s (signal propagation speed)\n",
        "PLATE_HALF = 0.15     # 30 cm half-size in meters\n",
        "SITES_N = 10          # 10 x 10 = 100 slots\n",
        "BALL_R  = 0.015       # 1.5 cm\n",
        "BALL_M  = 0.03        # 30 g\n",
        "\n",
        "# ---- 상단에 추가/수정 ----\n",
        "N_BINS = 2\n",
        "BINS   = np.array([0.0, 1.0], dtype=np.float32)  # {0, 1}\n",
        "\n",
        "def idx_to_u4(idx, n_bins=N_BINS, bins=BINS):\n",
        "    u = np.empty(4, dtype=np.float32)\n",
        "    for k in range(4):\n",
        "        u[k] = bins[idx % n_bins]\n",
        "        idx //= n_bins\n",
        "    # [U, R, D, L] 순서 유지\n",
        "    return u\n",
        "\n",
        "\n",
        "# ---- Delay line for sensor pipeline ----\n",
        "class DelayLine:\n",
        "    def __init__(self, max_steps, n_channels):\n",
        "        self.buf = np.zeros((max_steps+1, n_channels), dtype=np.float32)\n",
        "        self.ptr = 0\n",
        "        self.max_steps = max_steps\n",
        "        self.n = n_channels\n",
        "    def push(self, x_t):  # x_t shape [n_channels]\n",
        "        self.buf[self.ptr, :] = x_t\n",
        "        self.ptr = (self.ptr + 1) % (self.max_steps+1)\n",
        "    def read_delayed(self, ks):  # ks shape [n_channels], per-channel delay steps\n",
        "        # gather diagonal indices with wrap-around\n",
        "        idx = (self.ptr - 1 - ks) % (self.max_steps+1)\n",
        "        return self.buf[idx, np.arange(self.n)]\n",
        "\n",
        "def manhattan_delay_steps(x, y, dt=DT, v=V):\n",
        "    d_man = abs(x) + abs(y)\n",
        "    return int(np.ceil(d_man / (v * dt)))\n",
        "\n",
        "def make_site_grid(n=SITES_N, half=PLATE_HALF, z=0.006):\n",
        "    xs = np.linspace(-half, half, n)\n",
        "    ys = np.linspace(-half, half, n)\n",
        "    pts = []\n",
        "    for i, y in enumerate(ys):\n",
        "        for j, x in enumerate(xs):\n",
        "            pts.append((x, y, z, f\"sn_i{i}_j{j}\"))\n",
        "    return pts  # [(x,y,z,name) * 100]\n",
        "\n",
        "def site_xml_lines(pts):\n",
        "    lines = []\n",
        "    for (x,y,z,name) in pts:\n",
        "        lines.append(\n",
        "            f'<site name=\"{name}\" pos=\"{x:.4f} {y:.4f} {z:.4f}\" '\n",
        "            f'size=\"0.002\" type=\"sphere\" rgba=\"0.2 0.8 0.2 0.5\"/>'\n",
        "        )\n",
        "    return \"\\n      \".join(lines)\n",
        "\n",
        "# 4 edge actuators -> 2 hinge torques\n",
        "def edge4_to_torque2(u4, w=(1.,1.,1.,1.), max_tau=(0.2, 0.2)):\n",
        "    uU, uR, uD, uL = u4\n",
        "    wU, wR, wD, wL = w\n",
        "    tau_y = (wU*uU - wD*uD) * max_tau[1]\n",
        "    tau_x = (wR*uR - wL*uL) * max_tau[0]\n",
        "    return float(np.clip(tau_x, -max_tau[0], max_tau[0])), float(np.clip(tau_y, -max_tau[1], max_tau[1]))\n",
        "\n",
        "# === 2) MJCF(XML) build (30cm square plate + ball, 2 hinge motors) ===\n",
        "# - timestep=0.1 ms, integrator=RK4\n",
        "# - friction triplet(tangential, torsional, rolling) tuned to avoid infinite rolling\n",
        "#   (굴림마찰 항목은 문서에 설명됨)\n",
        "sites = make_site_grid()\n",
        "xml = f\"\"\"\n",
        "<mujoco model=\"tilt_plate\">\n",
        "  <compiler angle=\"degree\" inertiafromgeom=\"true\"/>\n",
        "  <option timestep=\"{DT:.7f}\" gravity=\"0 0 -9.81\" integrator=\"RK4\"/>\n",
        "  <default>\n",
        "    <geom  condim=\"6\" margin=\"0.001\" solimp=\"0.9 0.95 0.001\" solref=\"0.002 1\"/>\n",
        "    <default class=\"plate\">\n",
        "      <geom type=\"box\" friction=\"0.8 0.003 0.001\" rgba=\"0.8 0.8 0.85 1\"/>\n",
        "    </default>\n",
        "    <default class=\"ball\">\n",
        "      <geom type=\"sphere\" friction=\"0.9 0.005 0.002\" rgba=\"0.9 0.3 0.3 1\"/>\n",
        "    </default>\n",
        "    <joint armature=\"0.002\" damping=\"0.1\" limited=\"true\"/>\n",
        "    <motor gear=\"1.0\" ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\"/>\n",
        "  </default>\n",
        "  <worldbody>\n",
        "    <body name=\"plate_base\" pos=\"0 0 0\">\n",
        "      <joint name=\"hinge_x\" type=\"hinge\" axis=\"1 0 0\" range=\"-5 5\"/>\n",
        "      <joint name=\"hinge_y\" type=\"hinge\" axis=\"0 1 0\" range=\"-5 5\"/>\n",
        "      <geom name=\"plate_geom\" class=\"plate\" size=\"{PLATE_HALF} {PLATE_HALF} 0.005\" mass=\"1.0\"/>\n",
        "      {site_xml_lines(sites)}\n",
        "    </body>\n",
        "    <body name=\"ball\" pos=\"0 0 {BALL_R+0.01:.4f}\">\n",
        "      <freejoint name=\"ball_free\"/>\n",
        "      <geom name=\"ball_geom\" class=\"ball\" size=\"{BALL_R}\" mass=\"{BALL_M}\"/>\n",
        "    </body>\n",
        "  </worldbody>\n",
        "  <actuator>\n",
        "    <motor name=\"mx\" joint=\"hinge_x\" gear=\"1\"/>\n",
        "    <motor name=\"my\" joint=\"hinge_y\" gear=\"1\"/>\n",
        "  </actuator>\n",
        "</mujoco>\n",
        "\"\"\".strip()\n",
        "\n",
        "# Build model from XML string (official API)\n",
        "model = mj.MjModel.from_xml_string(xml)  # docs show this factory method\n",
        "data  = mj.MjData(model)\n",
        "\n",
        "# Precompute per-site manhattan delay (receiver @ center (0,0))\n",
        "site_xy = [(x, y) for (x,y,_,_) in sites]\n",
        "ks = np.array([manhattan_delay_steps(x, y) for (x,y) in site_xy], dtype=np.int32)\n",
        "delay_buf = DelayLine(max_steps=int(ks.max()), n_channels=len(sites))\n",
        "\n",
        "print(\"Model OK. Sites:\", len(sites), \"Max delay steps:\", ks.max())\n",
        "\n",
        "# === 3) Minimal Gymnasium-style Env wrapping MuJoCo ===\n",
        "# - obs: [ball_x, ball_y, ball_vx, ball_vy, hinge_x, hinge_y, hinge_xvel, hinge_yvel] + delayed 100-d sensor\n",
        "# - act: 4-d edge actuator in [-1,1], mapped to 2 hinge torques\n",
        "# - reward: -distance - 0.1*speed - 0.001*||u||^2\n",
        "@dataclass\n",
        "class PlateConfig:\n",
        "    dt: float = DT\n",
        "    substeps: int = 1\n",
        "    max_steps: int = 3000   # 0.3 s per episode at 0.1 ms\n",
        "    tau_max_x: float = 0.2\n",
        "    tau_max_y: float = 0.2\n",
        "\n",
        "# 맨 위에(선택): from ray.rllib.env.env_context import EnvContext\n",
        "\n",
        "class TiltPlateEnv(Env):\n",
        "    metadata = {\"render_modes\": []}\n",
        "\n",
        "    def __init__(self, env_config=None):\n",
        "        # 1) EnvContext/dict → 평범한 dict로 변환\n",
        "        try:\n",
        "            from ray.rllib.env.env_context import EnvContext  # optional import\n",
        "            if isinstance(env_config, EnvContext):\n",
        "                env_config = dict(env_config)\n",
        "        except Exception:\n",
        "            pass\n",
        "        if env_config is None:\n",
        "            env_config = {}\n",
        "\n",
        "        # 2) 기본값과 merge\n",
        "        self.cfg = PlateConfig(\n",
        "            dt       = env_config.get(\"dt\", DT),        # 0.1 ms\n",
        "            substeps = env_config.get(\"substeps\", 1),\n",
        "            max_steps= env_config.get(\"max_steps\", 3000),\n",
        "            tau_max_x= env_config.get(\"tau_max_x\", 0.2),\n",
        "            tau_max_y= env_config.get(\"tau_max_y\", 0.2),\n",
        "        )\n",
        "\n",
        "        # ---- 기존 초기화 그대로 (model/data/delay 등) ----\n",
        "        self.model = mj.MjModel.from_xml_string(xml)\n",
        "        self.data  = mj.MjData(self.model)\n",
        "        self.n_sites = len(sites)\n",
        "        self.ks = np.array([manhattan_delay_steps(x, y) for (x,y) in site_xy], dtype=np.int32)\n",
        "        self.delay = DelayLine(int(self.ks.max()), self.n_sites)\n",
        "\n",
        "        # ✅ DQN용 이산 액션(비음수 {0,1}^4 → 16개)\n",
        "        self.action_space = spaces.Discrete(N_BINS ** 4)  # N_BINS=2, BINS=[0.,1.]\n",
        "        high = np.full((8 + self.n_sites,),  np.inf, dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
        "        self.step_count = 0\n",
        "        self._reset_ball()\n",
        "\n",
        "    def _reset_ball(self):\n",
        "        # randomize ball initial pos near center\n",
        "        self.data.qpos[:] = 0.0\n",
        "        self.data.qvel[:] = 0.0\n",
        "        # freejoint qpos layout: [x,y,z, qw,qx,qy,qz]\n",
        "        self.data.qpos[0] = np.random.uniform(-0.03, 0.03)\n",
        "        self.data.qpos[1] = np.random.uniform(-0.03, 0.03)\n",
        "        self.data.qpos[2] = BALL_R + 0.005\n",
        "        # clear delays\n",
        "        self.delay = DelayLine(int(self.ks.max()), self.n_sites)\n",
        "\n",
        "    def _sense_now(self):\n",
        "        # simple radial basis around ball -> site signal in [0,1]\n",
        "        bx, by = self.data.qpos[0], self.data.qpos[1]\n",
        "        sig = []\n",
        "        for (x,y) in site_xy:\n",
        "            d = abs(bx - x) + abs(by - y)  # Manhattan for locality\n",
        "            val = math.exp(-d / 0.05)      # 5 cm falloff\n",
        "            sig.append(val)\n",
        "        return np.array(sig, dtype=np.float32)\n",
        "\n",
        "    def _obs(self):\n",
        "        bx, by = self.data.qpos[0], self.data.qpos[1]\n",
        "        vx, vy = self.data.qvel[0], self.data.qvel[1]\n",
        "        # hinges are last 2 joints (x,y)\n",
        "        # joint positions follow hinge_x, hinge_y after freejoint (7 dofs)\n",
        "        # mj identifies by name safer:\n",
        "        jx = mj.mj_name2id(self.model, mj.mjtObj.mjOBJ_JOINT, \"hinge_x\")\n",
        "        jy = mj.mj_name2id(self.model, mj.mjtObj.mjOBJ_JOINT, \"hinge_y\")\n",
        "        # qpos/qvel indexing known from model.jnt_qposadr\n",
        "        hx = self.data.qpos[self.model.jnt_qposadr[jx]]\n",
        "        hy = self.data.qpos[self.model.jnt_qposadr[jy]]\n",
        "        hvx = self.data.qvel[self.model.jnt_dofadr[jx]]\n",
        "        hvy = self.data.qvel[self.model.jnt_dofadr[jy]]\n",
        "        # delayed sensors:\n",
        "        delayed = self.delay.read_delayed(self.ks)\n",
        "        return np.concatenate([[bx,by,vx,vy,hx,hy,hvx,hvy], delayed]).astype(np.float32)\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self._reset_ball()\n",
        "        self.step_count = 0\n",
        "        # push a few zeros to delay line\n",
        "        self.delay.push(np.zeros(self.n_sites, dtype=np.float32))\n",
        "        return self._obs(), {}\n",
        "\n",
        "    # step() 안\n",
        "    def step(self, action):\n",
        "        u4 = idx_to_u4(int(action))   # {0,1}^4\n",
        "        tau_x, tau_y = edge4_to_torque2(\n",
        "            u4, max_tau=(self.cfg.tau_max_x, self.cfg.tau_max_y)\n",
        "        )\n",
        "        self.data.ctrl[0] = tau_x\n",
        "        self.data.ctrl[1] = tau_y\n",
        "        # 이하 동일...\n",
        "\n",
        "\n",
        "        # sense current & push into delay buffer\n",
        "        self.delay.push(self._sense_now())\n",
        "\n",
        "        # advance physics\n",
        "        for _ in range(self.cfg.substeps):\n",
        "            mj.mj_step(self.model, self.data)\n",
        "\n",
        "        bx, by = self.data.qpos[0], self.data.qpos[1]\n",
        "        vx, vy = self.data.qvel[0], self.data.qvel[1]\n",
        "        r = math.sqrt(bx*bx + by*by)\n",
        "        speed = math.sqrt(vx*vx + vy*vy)\n",
        "\n",
        "        # reward shaping: center & slow preferred; small action penalty\n",
        "        reward = -r - 0.1*speed - 0.001*float(np.sum(np.square(u4)))\n",
        "\n",
        "        self.step_count += 1\n",
        "        terminated = self.step_count >= self.cfg.max_steps\n",
        "        truncated  = False\n",
        "        return self._obs(), reward, terminated, truncated, {}\n",
        "\n",
        "# quick sanity: random rollout\n",
        "env = TiltPlateEnv()\n",
        "obs, _ = env.reset(seed=42)\n",
        "for t in range(50):\n",
        "    a = env.action_space.sample()\n",
        "    obs, rew, term, trunc, _ = env.step(a)\n",
        "    if term or trunc: break\n",
        "print(\"Random rollout OK. step:\", t+1, \"obs_dim:\", obs.shape)\n",
        "\n",
        "# === 4) (Optional) SpikingJelly Rate/Poisson Encoding Hook ===\n",
        "# - obs ∈ R^(108). 간단히 [0,1]로 정규화→포아송 스파이크로 변환 가능.\n",
        "# - 아래는 자리만들기(실험 때 연결). 자세한 인코딩은 SpikingJelly 튜토리얼 참고.\n",
        "def poisson_spike(obs, rate_scale=50.0, T=5, dt=1e-4):\n",
        "    x = np.asarray(obs, dtype=np.float32)\n",
        "\n",
        "    # peak-to-peak 범위 (NumPy 2.0+: ndarray.ptp() 대신 np.ptp(x) 사용)\n",
        "    rng = float(np.ptp(x))  # == x.max() - x.min()\n",
        "\n",
        "    # 모든 값이 같거나 비정상 값인 경우 대비\n",
        "    if not np.isfinite(rng) or rng < 1e-12:\n",
        "        x_norm = np.zeros_like(x, dtype=np.float32)\n",
        "    else:\n",
        "        x_norm = (x - float(x.min())) / (rng + 1e-6)\n",
        "\n",
        "    lam = x_norm * rate_scale * dt  # 기대 스파이크율\n",
        "    # T 스텝 동안 포아송 샘플링\n",
        "    return (np.random.rand(T, x.shape[0]) < lam).astype(np.float32)\n",
        "\n",
        "\n",
        "spk = poisson_spike(obs, T=5)\n",
        "print(\"Poisson spikes shape:\", spk.shape)\n",
        "\n",
        "# === 5) RLlib DQN minimal run (tiny) ===\n",
        "# - Gymnasium Env을 RLlib에 직접 넘김.\n",
        "# - 빠른 데모: train_iters=1 (실전은 늘려야 함)\n",
        "# %%\n",
        "ray.shutdown()\n",
        "ray.init(ignore_reinit_error=True, include_dashboard=False)\n",
        "\n",
        "\n",
        "cfg = (\n",
        "    DQNConfig()\n",
        "    .environment(env=TiltPlateEnv, env_config={\n",
        "            \"tau_max_x\": 0.2,\n",
        "            \"tau_max_y\": 0.2,\n",
        "            # 필요하면 dt/substeps 등도 여기서 조정\n",
        "        },)  # ← 클래스로!\n",
        "    .framework(\"torch\")\n",
        "    .api_stack(enable_rl_module_and_learner=True)\n",
        "    .env_runners(num_env_runners=0)               # ← 예전 rollouts 대체\n",
        ")\n",
        "\n",
        "algo = cfg.build()\n",
        "res = algo.train()\n",
        "print({\n",
        "    \"len_mean\": res.get(\"env_runners\", {}).get(\"episode_len_mean\"),\n",
        "    \"ret_mean\": res.get(\"env_runners\", {}).get(\"episode_return_mean\"),\n",
        "})\n",
        "\n",
        "\n",
        "print(\"DQN one-iter result keys:\", {k: res[k] for k in [\"episode_len_mean\", \"episode_reward_mean\"] if k in res})\n",
        "algo.stop()\n",
        "ray.shutdown()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title @dataclass SimulationConfig\n",
        "\n",
        "@dataclass\n",
        "class SimulationConfig:\n",
        "    # --- 기본 시뮬레이션 ---\n",
        "    dt_ms: float = 0.1                              # [ms]\n",
        "    dt_sim : float = dt_ms / 1000\n",
        "    plane_size: float = 0.30                        # [m] 한 변 길이 (±0.15 m)\n",
        "    goal_r: float = 0.005                           # [m] 목표 반경\n",
        "    tau_pre_ms: float = 5.0\n",
        "    tau_post_ms: float = 5.0\n",
        "    tau_e_ms: float = 50.0\n",
        "    eta: float = 1e-3\n",
        "    max_dw_per_step: float = 1e-3\n",
        "    scale_every: int = 100\n",
        "    use_mf_collat: bool = True\n",
        "    mf_collat_gain: float = 0.3\n",
        "    seed: int = 42\n",
        "\n",
        "    # --- SNN / 센서 ---\n",
        "    n_mf: int = 100                                 # 센서(모스피버, etc.) 개수\n",
        "    sensor_slots_xy: int = 10                       # 10x10 = 100 슬롯\n",
        "    conduction_vel: float = 50.0                    # [m/s] 신경 신호 속도\n",
        "    use_manhattan_delay: bool = True                # 맨해튼 거리를 쓸 건가요?\n",
        "\n",
        "    # --- 뉴런 계층 ---\n",
        "    n_mf: int = 100\n",
        "    n_grc: int = 1024\n",
        "    n_pkj: int = 64\n",
        "    n_motor: int = 4                                # +X, -X, +Y, -Y\n",
        "\n",
        "    # --- 물리/액추에이터(고정 파라미터) ---\n",
        "    plate_mass: float = 0.50                        # [kg] 예: 0.5 kg\n",
        "    actuator_torque_scale: float = 0.05             # [N·m] 스칼라 가중치\n",
        "    friction_coeff: float = 0.05                    # 무차원/단순 감쇠항\n",
        "    gravity: float = 9.8\n",
        "    # 난수 고정(슬롯 샘플링 재현성)\n",
        "    seed: int = 42\n",
        "\n",
        "    #inputsimconfig\n",
        "\n",
        "    # 시뮬레이션\n",
        "    # 발화(rate-based Poisson)\n",
        "    base_rate_hz: float = 400.0\n",
        "    rate_scale_weight: float = 40.0      # 가중: lam = base + k*weight_g\n",
        "    sigma_ratio: float = 1.0             # σ = ratio * ball_radius\n",
        "    # 판 기울기 제한\n",
        "    max_tilt_deg: float = 5.0\n",
        "    # 비디오/시각화\n",
        "    video_fps: int = 30\n",
        "\n",
        "    # --- 4-엣지 액추에이터 매핑(가상) ---\n",
        "    edge_ctrl_min: float = 0.0   # 각 채널 입력 하한(수축 0~1 가정)\n",
        "    edge_ctrl_max: float = 1.0   # 각 채널 입력 상한\n",
        "    edge_gain_deg: float = 10.0   # (우-좌) 또는 (상-하) 1.0 차이가 만드는 기울기[deg]\n",
        "    edge_x_sign: float = +1.0    # 부호 교정(축 정의에 따라 필요시 -1)\n",
        "    edge_y_sign: float = +1.0\n",
        "\n",
        "    # 안정화\n",
        "    settle_steps: int = 500"
      ],
      "metadata": {
        "id": "JEVkjb90L0P1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cerebellar Modification\n",
        "\n",
        "# ---- 4) 소뇌형 SNN (SpikingJelly 캡슐화) -------------------------------------\n",
        "from typing import Optional, Tuple, Protocol\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import ceil\n",
        "from spikingjelly.activation_based import layer, neuron, functional\n",
        "from spikingjelly.activation_based import learning, surrogate\n",
        "\n",
        "class ISNN(Protocol):\n",
        "    def reset(self) -> None: ...\n",
        "    def forward(self, mf_spikes: np.ndarray) -> np.ndarray: ... # -> motor spikes (4,)\n",
        "    def learn(self, cf_signal: float) -> None: ...\n",
        "\n",
        "class CerebellarNet(ISNN):\n",
        "    \"\"\"\n",
        "    내부 구현은 SpikingJelly activation_based(IFNode, Linear) 사용.  :contentReference[oaicite:8]{index=8}\n",
        "    forward:  MF -> GrC -> PkG -> Motor(4) 스파이크\n",
        "    learn:    PF(GrC)-Motor 가중치에 3-요소 규칙(eligibility × CF) 적용\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: SimulationConfig, device: Optional[str] = None):\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.torch = torch; self.nn = nn;\n",
        "        self.sj_func = functional       #spikingjelly functional\n",
        "        self.dev = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # ---------- Fixed expansion: MF -> GrC ----------\n",
        "        self.mf2grc = layer.Linear(cfg.n_mf, cfg.n_grc, bias=False).to(self.dev)\n",
        "        with self.torch.no_grad():\n",
        "            W = self.torch.zeros(cfg.n_grc, cfg.n_mf, device=self.dev)\n",
        "            mask = (self.torch.rand_like(W) < 0.1).float()\n",
        "            W += mask\n",
        "            W /= (W.sum(dim=1, keepdim=True) + 1e-6)\n",
        "            self.mf2grc.weight.copy_(W)\n",
        "        for p in self.mf2grc.parameters(): p.requires_grad_(False)\n",
        "        self.grc = neuron.IFNode(v_threshold=1.0, v_reset=0.0, detach_reset=True).to(self.dev)\n",
        "\n",
        "        # ---------- Trainable plastic site: GrC(PF) -> PKJ ----------\n",
        "        self.grc2pkj = layer.Linear(cfg.n_grc, cfg.n_pkj, bias=False).to(self.dev)\n",
        "        self.nn.init.zeros_(self.grc2pkj.weight)  # start neutral; learn via CF-gated 3-factor\n",
        "        self.pkj = neuron.IFNode(v_threshold=1.0, v_reset=0.0, detach_reset=True).to(self.dev)\n",
        "\n",
        "        tau_pre_steps  = float(self.cfg.tau_pre_ms / self.cfg.dt_ms)\n",
        "        tau_post_steps = float(self.cfg.tau_post_ms / self.cfg.dt_ms)\n",
        "        def _f_w(x: torch.Tensor) -> torch.Tensor:\n",
        "            return torch.clamp(x, -1.5, 1.5)\n",
        "        self.stdp_pkj = learning.STDPLearner(\n",
        "            step_mode='s',\n",
        "            synapse=self.grc2pkj,    # 학습할 Linear\n",
        "            sn=self.pkj,             # post 뉴런층\n",
        "            tau_pre=tau_pre_steps,\n",
        "            tau_post=tau_post_steps,\n",
        "            f_pre=_f_w,\n",
        "            f_post=_f_w\n",
        "        )\n",
        "        self.opt_stdp = torch.optim.SGD(self.grc2pkj.parameters(), lr=self.cfg.eta, momentum=0.0)\n",
        "\n",
        "        # ---------- CF input: strong drive to PKJ (one-to-one by default) ----------\n",
        "        self.cf2pkj = layer.Linear(cfg.n_pkj, cfg.n_pkj, bias=False).to(self.dev)\n",
        "        with self.torch.no_grad():\n",
        "            self.cf2pkj.weight.copy_(self.torch.eye(cfg.n_pkj, device=self.dev) * 3.0)  # strong drive\n",
        "        for p in self.cf2pkj.parameters(): p.requires_grad_(False)\n",
        "\n",
        "        # ---------- DN/motor: receives (-) PKJ, (+) MF collaterals (optional) ----------\n",
        "        self.pkj2mo = layer.Linear(cfg.n_pkj, cfg.n_motor, bias=False).to(self.dev)\n",
        "        with self.torch.no_grad():\n",
        "            # inhibitory mapping: negative weights, broadly tuned\n",
        "            Wp = - self.torch.randn(cfg.n_motor, cfg.n_pkj, device=self.dev)\n",
        "            Wp /= (Wp.std(dim=1, keepdim=True) + 1e-6)\n",
        "            self.pkj2mo.weight.copy_(Wp * 0.2)\n",
        "        for p in self.pkj2mo.parameters(): p.requires_grad_(False)\n",
        "\n",
        "        if cfg.use_mf_collat:\n",
        "            self.mf2mo = layer.Linear(cfg.n_mf, cfg.n_motor, bias=False).to(self.dev)\n",
        "            with self.torch.no_grad():\n",
        "                Wc = self.torch.zeros(cfg.n_motor, cfg.n_mf, device=self.dev)\n",
        "                mask = (self.torch.rand_like(Wc) < 0.2).float()\n",
        "                Wc += mask\n",
        "                Wc /= (Wc.sum(dim=1, keepdim=True) + 1e-6)\n",
        "                self.mf2mo.weight.copy_(Wc * cfg.mf_collat_gain)\n",
        "            for p in self.mf2mo.parameters(): p.requires_grad_(False)\n",
        "        else:\n",
        "            self.mf2mo = None\n",
        "\n",
        "        self.mo  = neuron.IFNode(v_threshold=1.0, v_reset=0.0, detach_reset=True).to(self.dev)\n",
        "\n",
        "        # ---------- Eligibility traces for PF->PKJ ----------\n",
        "        self.pre_tr  = np.zeros(cfg.n_grc, dtype=np.float32)     # PF (pre)\n",
        "        self.post_tr = np.zeros(cfg.n_pkj, dtype=np.float32)     # PKJ (post)\n",
        "        self.E = np.zeros((cfg.n_grc, cfg.n_pkj), dtype=np.float32)\n",
        "\n",
        "        self.pre_decay  = math.exp(-cfg.dt_ms/cfg.tau_pre_ms)\n",
        "        self.post_decay = math.exp(-cfg.dt_ms/cfg.tau_post_ms)\n",
        "        self.e_decay    = math.exp(-cfg.dt_ms/cfg.tau_e_ms)\n",
        "\n",
        "        # cache last spikes for learning step\n",
        "        self._last = {}\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self.sj_func.reset_net(self.mf2grc); self.sj_func.reset_net(self.grc)\n",
        "        self.sj_func.reset_net(self.grc2pkj); self.sj_func.reset_net(self.pkj)\n",
        "        self.sj_func.reset_net(self.pkj2mo);  self.sj_func.reset_net(self.mo)\n",
        "        if self.mf2mo is not None: self.sj_func.reset_net(self.mf2mo)\n",
        "        self.pre_tr[:] = 0; self.post_tr[:] = 0; self.E[:] = 0\n",
        "        self._last.clear()\n",
        "\n",
        "    def forward(self, mf_spikes: np.ndarray, cf_spikes: Optional[np.ndarray] = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        mf_spikes : (N_MF,)  binary\n",
        "        cf_spikes : (N_PKJ,) binary or rate-like; if None, zeros\n",
        "        returns   : (N_MOTOR,) binary spikes\n",
        "        \"\"\"\n",
        "        t = self.torch\n",
        "        x_mf = t.from_numpy(mf_spikes)[None,:].to(self.dev)\n",
        "        x_cf = None\n",
        "        if cf_spikes is not None:\n",
        "            x_cf = t.from_numpy(cf_spikes)[None,:].to(self.dev)\n",
        "\n",
        "        grc_spk = self.grc(self.mf2grc(x_mf))\n",
        "        pkj_in  = self.grc2pkj(grc_spk)\n",
        "        if x_cf is not None:\n",
        "            pkj_in = pkj_in + self.cf2pkj(x_cf)   # CF drives PKJ strongly\n",
        "        pkj_spk = self.pkj(pkj_in)\n",
        "\n",
        "        mo_in = self.pkj2mo(pkj_spk)              # inhibition by PKJ\n",
        "        if (self.mf2mo is not None):\n",
        "            mo_in = mo_in + self.mf2mo(x_mf)      # MF collateral excitation\n",
        "        mo_spk = self.mo(mo_in)\n",
        "\n",
        "        # cache last spikes for learning\n",
        "        self._last['grc'] = grc_spk.detach().cpu().numpy()[0]\n",
        "        self._last['pkj'] = pkj_spk.detach().cpu().numpy()[0]\n",
        "        self._last['mo']  = mo_spk.detach().cpu().numpy()[0]\n",
        "        self._last['cf']  = (cf_spikes.copy() if cf_spikes is not None else None)\n",
        "        return self._last['mo']\n",
        "\n",
        "    def learn(self, cf_signal: Optional[np.ndarray] = None) -> None:\n",
        "        \"\"\"\n",
        "        CF-gated three-factor at PF->PKJ:\n",
        "          ΔW_ij ∝ η * (CF_j) * E_ij\n",
        "        cf_signal:\n",
        "          - None or scalar → uses scalar for all PKJ units\n",
        "          - (N_PKJ,) vector → per-PKJ gating (recommended if you pass cf_spikes)\n",
        "        \"\"\"\n",
        "        grc_out = self._last['grc']; pkj_out = self._last['pkj']\n",
        "        # 1) update traces\n",
        "        self.pre_tr  = self.pre_tr  * self.pre_decay  + grc_out\n",
        "        self.post_tr = self.post_tr * self.post_decay + pkj_out\n",
        "        self.E = self.E * self.e_decay + (np.outer(self.pre_tr, pkj_out) - np.outer(grc_out, self.post_tr))\n",
        "\n",
        "        # 2) CF gating (vector per PKJ if available)\n",
        "        if cf_signal is None:\n",
        "            if self._last.get('cf') is not None:\n",
        "                cf = self._last['cf']           # (N_PKJ,)\n",
        "            else:\n",
        "                cf = 1.0                        # scalar gate\n",
        "        else:\n",
        "            cf = cf_signal                      # scalar or (N_PKJ,)\n",
        "\n",
        "        # PATCH: in CerebellarNet.learn(), just before applying to weight:\n",
        "        with self.torch.no_grad():\n",
        "            E_t = self.torch.from_numpy(self.E).to(self.dev)     # [N_GRC, N_PKJ]\n",
        "            if np.isscalar(cf):\n",
        "                dW = self.cfg.eta * float(cf) * E_t\n",
        "            else:\n",
        "                cf_vec = self.torch.from_numpy(np.asarray(cf, dtype=np.float32)).to(self.dev)  # [N_PKJ]\n",
        "                dW = self.cfg.eta * (E_t * cf_vec)   # broadcast on last dim\n",
        "\n",
        "            # --- NEW: per-step update clamp (stability) ---\n",
        "            dW.clamp_(-self.cfg.max_dw_per_step, self.cfg.max_dw_per_step)\n",
        "\n",
        "            self.grc2pkj.weight += dW.T              # [N_PKJ, N_GRC]\n",
        "            self.grc2pkj.weight.clamp_(-1.5, 1.5)\n",
        "\n",
        "            # --- NEW: periodic synaptic scaling (row L2 ≤ 1) ---\n",
        "            if not hasattr(self, \"_scale_k\"):\n",
        "                self._scale_k = 0\n",
        "            self._scale_k += 1\n",
        "            if (self._scale_k % self.cfg.scale_every) == 0:\n",
        "                W = self.grc2pkj.weight\n",
        "                rownorm = W.norm(p=2, dim=1, keepdim=True).clamp_min(1.0)\n",
        "                W.mul_(1.0 / rownorm)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "c0udQblhLwUU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== SNN <-> Env 어댑터 ====\n",
        "\n",
        "# 0) MF 인코더: obs -> Poisson spikes (한 스텝분)\n",
        "def encode_mf_spikes(obs, rate_scale=50.0, dt=DT):\n",
        "    x = np.asarray(obs, dtype=np.float32)\n",
        "    # 간단 정규화: [0,1]\n",
        "    rng = float(np.ptp(x))\n",
        "    x = (x - float(x.min())) / (rng + 1e-6) if rng >= 1e-12 else np.zeros_like(x, dtype=np.float32)\n",
        "    lam = x * rate_scale * dt\n",
        "    return (np.random.rand(x.shape[0]) < lam).astype(np.float32)  # (N_MF,)\n",
        "\n",
        "# 1) motor(4) 스파이크 -> 엣지 명령 u4 (비음수만: {0,1})\n",
        "def motor_spikes_to_u4(mo_spk):\n",
        "    # 이미 binary니까 그대로 uU,uR,uD,uL\n",
        "    return mo_spk.astype(np.float32)  # shape (4,), 값 {0,1}\n",
        "\n",
        "# 2) CF 게이트 설계: 거리 감소는 \"좋음(+)\", 증가 \"나쁨(-)\"\n",
        "#    - 스칼라 CF로 간단히 시작 (PKJ 전체에 동일 게이트)\n",
        "def cf_from_transition(prev_r, curr_r, k_pos=+1.0, k_neg=-1.0):\n",
        "    dr = curr_r - prev_r\n",
        "    if dr < 0:\n",
        "        return k_pos  # 가까워지면 강화(+)\n",
        "    elif dr > 0:\n",
        "        return k_neg  # 멀어지면 약화(-)\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "# 3) 한 에피소드 학습 루프 (SNN만)\n",
        "def train_one_episode_snn(env, cereb, steps=2000, rate_scale=50.0):\n",
        "    obs, _ = env.reset()\n",
        "    cereb.reset()\n",
        "\n",
        "    # prev_r 초기화\n",
        "    bx, by = env.data.qpos[0], env.data.qpos[1]\n",
        "    prev_r = float(np.hypot(bx, by))\n",
        "    ep_ret = 0.0\n",
        "\n",
        "    for t in range(steps):\n",
        "        # --- 인코딩 & forward ---\n",
        "        mf_spk = encode_mf_spikes(obs, rate_scale=rate_scale, dt=DT)               # (N_MF,)\n",
        "        mo_spk = cereb.forward(mf_spk, cf_spikes=None)                             # (4,)\n",
        "\n",
        "        # --- 행동 적용 ---\n",
        "        u4 = motor_spikes_to_u4(mo_spk)                                           # {0,1}^4\n",
        "        tau_x, tau_y = edge4_to_torque2(u4, max_tau=(env.cfg.tau_max_x, env.cfg.tau_max_y))\n",
        "        env.data.ctrl[0] = tau_x; env.data.ctrl[1] = tau_y\n",
        "\n",
        "        # --- 환경 스텝 ---\n",
        "        obs, reward, term, trunc, _ = env.step(int(0))  # Gym checker를 통과하려면 더미 action 필요 없음\n",
        "        # ↑ 우리 env.step(action) 인터페이스가 DQN/Discrete 기준이라면,\n",
        "        #   여기선 이미 data.ctrl에 토크를 넣었으니, env.step은 \"no-op\" action으로 호출해 한 스텝 전진만 수행.\n",
        "        #   (필요하다면 step_internal() 같은 헬퍼를 분리해도 됨)\n",
        "\n",
        "        # --- CF 계산 & 학습 ---\n",
        "        bx, by = env.data.qpos[0], env.data.qpos[1]\n",
        "        curr_r = float(np.hypot(bx, by))\n",
        "        cf = cf_from_transition(prev_r, curr_r, k_pos=+1.0, k_neg=-1.0)\n",
        "        cereb.learn(cf_signal=cf)\n",
        "\n",
        "        prev_r = curr_r\n",
        "        ep_ret += reward\n",
        "        if term or trunc:\n",
        "            break\n",
        "    return ep_ret, t+1\n"
      ],
      "metadata": {
        "id": "4VBdhDJJLkUZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 0) Imports & constants ====\n",
        "import math, numpy as np, textwrap\n",
        "import mujoco as mj\n",
        "from gymnasium import Env, spaces\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "# Ray/RLlib는 뒤의 가이드에서 사용 (여기선 임포트 지연 가능)\n",
        "\n",
        "# 시뮬 타임/지연 설정\n",
        "DT = 1e-4   # 0.1 ms\n",
        "V  = 50.0   # m/s\n",
        "PLATE_HALF = 0.15\n",
        "SITES_N = 10\n",
        "BALL_R  = 0.015\n",
        "BALL_M  = 0.03\n",
        "\n",
        "# ==== 1) Delay line + grid sites ====\n",
        "class DelayLine:\n",
        "    def __init__(self, max_steps, n_channels):\n",
        "        self.buf = np.zeros((max_steps+1, n_channels), dtype=np.float32)\n",
        "        self.ptr = 0\n",
        "        self.max_steps = max_steps\n",
        "        self.n = n_channels\n",
        "    def push(self, x_t):\n",
        "        self.buf[self.ptr, :] = x_t\n",
        "        self.ptr = (self.ptr + 1) % (self.max_steps+1)\n",
        "    def read_delayed(self, ks):\n",
        "        idx = (self.ptr - 1 - ks) % (self.max_steps+1)\n",
        "        return self.buf[idx, np.arange(self.n)]\n",
        "\n",
        "def manhattan_delay_steps(x, y, dt=DT, v=V):\n",
        "    return int(np.ceil((abs(x)+abs(y)) / (v*dt)))\n",
        "\n",
        "def make_site_grid(n=SITES_N, half=PLATE_HALF, z=0.006):\n",
        "    xs = np.linspace(-half, half, n)\n",
        "    ys = np.linspace(-half, half, n)\n",
        "    pts = []\n",
        "    for i, y in enumerate(ys):\n",
        "        for j, x in enumerate(xs):\n",
        "            pts.append((x, y, z, f\"sn_i{i}_j{j}\"))\n",
        "    return pts\n",
        "\n",
        "def site_xml_lines(pts):\n",
        "    lines = []\n",
        "    for (x,y,z,name) in pts:\n",
        "        lines.append(\n",
        "            f'<site name=\"{name}\" pos=\"{x:.4f} {y:.4f} {z:.4f}\" '\n",
        "            f'size=\"0.002\" type=\"sphere\" rgba=\"0.2 0.8 0.2 0.5\"/>'\n",
        "        )\n",
        "    return \"\\n      \".join(lines)\n",
        "\n",
        "# 4-edge → 2-hinge torque\n",
        "def edge4_to_torque2(u4, w=(1.,1.,1.,1.), max_tau=(0.2,0.2)):\n",
        "    uU, uR, uD, uL = u4\n",
        "    wU, wR, wD, wL = w\n",
        "    tau_y = (wU*uU - wD*uD) * max_tau[1]\n",
        "    tau_x = (wR*uR - wL*uL) * max_tau[0]\n",
        "    return float(np.clip(tau_x, -max_tau[0], max_tau[0])), float(np.clip(tau_y, -max_tau[1], max_tau[1]))\n",
        "\n",
        "# ==== 2) MJCF XML build ====\n",
        "sites = make_site_grid()\n",
        "xml = f\"\"\"\n",
        "<mujoco model=\"tilt_plate\">\n",
        "  <compiler angle=\"degree\" inertiafromgeom=\"true\"/>\n",
        "  <option timestep=\"{DT:.7f}\" gravity=\"0 0 -9.81\" integrator=\"RK4\"/>\n",
        "  <default>\n",
        "    <geom  condim=\"6\" margin=\"0.001\" solimp=\"0.9 0.95 0.001\" solref=\"0.002 1\"/>\n",
        "    <default class=\"plate\">\n",
        "      <geom type=\"box\" friction=\"0.8 0.003 0.001\" rgba=\"0.8 0.8 0.85 1\"/>\n",
        "    </default>\n",
        "    <default class=\"ball\">\n",
        "      <geom type=\"sphere\" friction=\"0.9 0.005 0.002\" rgba=\"0.9 0.3 0.3 1\"/>\n",
        "    </default>\n",
        "    <joint armature=\"0.002\" damping=\"0.1\" limited=\"true\"/>\n",
        "    <motor gear=\"1.0\" ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\"/>\n",
        "  </default>\n",
        "  <worldbody>\n",
        "    <body name=\"plate_base\" pos=\"0 0 0\">\n",
        "      <joint name=\"hinge_x\" type=\"hinge\" axis=\"1 0 0\" range=\"-5 5\"/>\n",
        "      <joint name=\"hinge_y\" type=\"hinge\" axis=\"0 1 0\" range=\"-5 5\"/>\n",
        "      <geom name=\"plate_geom\" class=\"plate\" size=\"{PLATE_HALF} {PLATE_HALF} 0.005\" mass=\"1.0\"/>\n",
        "      {site_xml_lines(sites)}\n",
        "    </body>\n",
        "    <body name=\"ball\" pos=\"0 0 {BALL_R+0.01:.4f}\">\n",
        "      <freejoint name=\"ball_free\"/>\n",
        "      <geom name=\"ball_geom\" class=\"ball\" size=\"{BALL_R}\" mass=\"{BALL_M}\"/>\n",
        "    </body>\n",
        "  </worldbody>\n",
        "  <actuator>\n",
        "    <motor name=\"mx\" joint=\"hinge_x\" gear=\"1\"/>\n",
        "    <motor name=\"my\" joint=\"hinge_y\" gear=\"1\"/>\n",
        "  </actuator>\n",
        "</mujoco>\n",
        "\"\"\".strip()\n",
        "\n",
        "# ==== 3) Env (EnvContext 호환) + 이산 액션 {0,1}^4 ====\n",
        "N_BINS = 2\n",
        "BINS   = np.array([0.0, 1.0], dtype=np.float32)\n",
        "def idx_to_u4(idx, n_bins=N_BINS, bins=BINS):\n",
        "    u = np.empty(4, dtype=np.float32)\n",
        "    for k in range(4):\n",
        "        u[k] = bins[idx % n_bins]\n",
        "        idx //= n_bins\n",
        "    return u  # [U,R,D,L]\n",
        "\n",
        "@dataclass\n",
        "class PlateConfig:\n",
        "    dt: float = DT\n",
        "    substeps: int = 1\n",
        "    max_steps: int = 3000\n",
        "    tau_max_x: float = 0.2\n",
        "    tau_max_y: float = 0.2\n",
        "\n",
        "class TiltPlateEnv(Env):\n",
        "    metadata = {\"render_modes\": []}\n",
        "    def __init__(self, env_config=None):\n",
        "        # EnvContext → dict\n",
        "        try:\n",
        "            from ray.rllib.env.env_context import EnvContext\n",
        "            if isinstance(env_config, EnvContext):\n",
        "                env_config = dict(env_config)\n",
        "        except Exception:\n",
        "            pass\n",
        "        if env_config is None:\n",
        "            env_config = {}\n",
        "        self.cfg = PlateConfig(\n",
        "            dt       = env_config.get(\"dt\", DT),\n",
        "            substeps = env_config.get(\"substeps\", 1),\n",
        "            max_steps= env_config.get(\"max_steps\", 3000),\n",
        "            tau_max_x= env_config.get(\"tau_max_x\", 0.2),\n",
        "            tau_max_y= env_config.get(\"tau_max_y\", 0.2),\n",
        "        )\n",
        "        # MuJoCo model/data\n",
        "        self.model = mj.MjModel.from_xml_string(xml)\n",
        "        self.data  = mj.MjData(self.model)\n",
        "        # delays\n",
        "        self.n_sites = len(sites)\n",
        "        self.site_xy = [(x,y) for (x,y,_,_) in sites]\n",
        "        self.ks = np.array([manhattan_delay_steps(x,y) for (x,y) in self.site_xy], dtype=np.int32)\n",
        "        self.delay = DelayLine(int(self.ks.max()), self.n_sites)\n",
        "        # action/obs\n",
        "        self.action_space = spaces.Discrete(N_BINS**4)  # 16\n",
        "        high = np.full((8 + self.n_sites,), np.inf, dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
        "        self.step_count = 0\n",
        "        self._reset_ball()\n",
        "\n",
        "    def _reset_ball(self):\n",
        "        self.data.qpos[:] = 0.0; self.data.qvel[:] = 0.0\n",
        "        self.data.qpos[0] = np.random.uniform(-0.03, 0.03)\n",
        "        self.data.qpos[1] = np.random.uniform(-0.03, 0.03)\n",
        "        self.data.qpos[2] = BALL_R + 0.005\n",
        "        self.delay = DelayLine(int(self.ks.max()), self.n_sites)\n",
        "\n",
        "    def _sense_now(self):\n",
        "        bx, by = self.data.qpos[0], self.data.qpos[1]\n",
        "        sig = []\n",
        "        for (x,y) in self.site_xy:\n",
        "            d = abs(bx-x) + abs(by-y)\n",
        "            sig.append(math.exp(-d/0.05))\n",
        "        return np.array(sig, dtype=np.float32)\n",
        "\n",
        "    def _obs(self):\n",
        "        bx, by = self.data.qpos[0], self.data.qpos[1]\n",
        "        vx, vy = self.data.qvel[0], self.data.qvel[1]\n",
        "        jx = mj.mj_name2id(self.model, mj.mjtObj.mjOBJ_JOINT, \"hinge_x\")\n",
        "        jy = mj.mj_name2id(self.model, mj.mjtObj.mjOBJ_JOINT, \"hinge_y\")\n",
        "        hx = self.data.qpos[self.model.jnt_qposadr[jx]]\n",
        "        hy = self.data.qpos[self.model.jnt_qposadr[jy]]\n",
        "        hvx = self.data.qvel[self.model.jnt_dofadr[jx]]\n",
        "        hvy = self.data.qvel[self.model.jnt_dofadr[jy]]\n",
        "        delayed = self.delay.read_delayed(self.ks)\n",
        "        return np.concatenate([[bx,by,vx,vy,hx,hy,hvx,hvy], delayed]).astype(np.float32)\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self._reset_ball()\n",
        "        self.step_count = 0\n",
        "        self.delay.push(np.zeros(self.n_sites, dtype=np.float32))\n",
        "        return self._obs(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        u4 = idx_to_u4(int(action))  # {0,1}^4\n",
        "        tau_x, tau_y = edge4_to_torque2(u4, max_tau=(self.cfg.tau_max_x, self.cfg.tau_max_y))\n",
        "        self.data.ctrl[0] = tau_x; self.data.ctrl[1] = tau_y\n",
        "        # push current sensors, step physics\n",
        "        self.delay.push(self._sense_now())\n",
        "        for _ in range(self.cfg.substeps):\n",
        "            mj.mj_step(self.model, self.data)\n",
        "        bx, by = self.data.qpos[0], self.data.qpos[1]\n",
        "        vx, vy = self.data.qvel[0], self.data.qvel[1]\n",
        "        r = math.hypot(bx, by)\n",
        "        speed = math.hypot(vx, vy)\n",
        "        reward = -r - 0.1*speed - 0.001*float(np.sum(u4*u4))  # 이완 유도\n",
        "        self.step_count += 1\n",
        "        terminated = self.step_count >= self.cfg.max_steps\n",
        "        truncated = False\n",
        "        return self._obs(), reward, terminated, truncated, {}\n",
        "\n",
        "# ==== 4) SNN ↔ Env 어댑터 (CerebellarNet 사용) ====\n",
        "def encode_mf_spikes(obs, rate_scale=60.0, dt=DT):\n",
        "    x = np.asarray(obs, dtype=np.float32)\n",
        "    rng = float(np.ptp(x))\n",
        "    x = (x - float(x.min())) / (rng + 1e-6) if rng >= 1e-12 else np.zeros_like(x, dtype=np.float32)\n",
        "    lam = x * rate_scale * dt\n",
        "    return (np.random.rand(x.shape[0]) < lam).astype(np.float32)\n",
        "\n",
        "def motor_spikes_to_u4(mo_spk):\n",
        "    return mo_spk.astype(np.float32)  # {0,1}^4\n",
        "\n",
        "def cf_from_transition(prev_r, curr_r, k_pos=+1.0, k_neg=-1.0):\n",
        "    dr = curr_r - prev_r\n",
        "    if dr < 0:  return k_pos\n",
        "    if dr > 0:  return k_neg\n",
        "    return 0.0\n",
        "\n",
        "def train_one_episode_snn(env: TiltPlateEnv, cereb, steps=2000, rate_scale=60.0):\n",
        "    obs, _ = env.reset()\n",
        "    cereb.reset()\n",
        "    bx, by = env.data.qpos[0], env.data.qpos[1]\n",
        "    prev_r = float(math.hypot(bx, by))\n",
        "    ep_ret = 0.0\n",
        "    for t in range(steps):\n",
        "        # forward with MF spikes\n",
        "        mf_spk = encode_mf_spikes(obs, rate_scale=rate_scale, dt=DT)\n",
        "        mo_spk = cereb.forward(mf_spk, cf_spikes=None)   # (4,) binary\n",
        "        # apply action\n",
        "        u4 = motor_spikes_to_u4(mo_spk)\n",
        "        tau_x, tau_y = edge4_to_torque2(u4, max_tau=(env.cfg.tau_max_x, env.cfg.tau_max_y))\n",
        "        env.data.ctrl[0] = tau_x; env.data.ctrl[1] = tau_y\n",
        "        # physics step (noop action since ctrl already set)\n",
        "        obs, reward, term, trunc, _ = env.step(0)\n",
        "        # CF and learning\n",
        "        bx, by = env.data.qpos[0], env.data.qpos[1]\n",
        "        curr_r = float(math.hypot(bx, by))\n",
        "        cf = cf_from_transition(prev_r, curr_r, k_pos=+1.0, k_neg=-1.0)\n",
        "        cereb.learn(cf_signal=cf)\n",
        "        prev_r = curr_r\n",
        "        ep_ret += reward\n",
        "        if term or trunc:\n",
        "            break\n",
        "    return ep_ret, t+1\n",
        "\n",
        "print(\"Model OK. Sites:\", len(sites), \"Max delay steps:\", int(np.max([manhattan_delay_steps(x,y) for (x,y,_,_) in sites])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvpqkZ0FOQfH",
        "outputId": "63370ebf-0a7c-499b-d151-e44dbd537221"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model OK. Sites: 100 Max delay steps: 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== SNN 단독 트레이닝 실행 ====\n",
        "# (네 CerebellarNet 클래스 정의가 끝난 뒤)\n",
        "cfg_sn = SimulationConfig(  # 네가 쓰는 dataclass에 맞춰 값 전달\n",
        "    n_mf=108, n_grc=512, n_pkj=8, n_motor=4,\n",
        "    tau_pre_ms=5.0, tau_post_ms=5.0, tau_e_ms=50.0,\n",
        "    dt_ms=0.1, eta=1e-3, max_dw_per_step=1e-3, scale_every=100,\n",
        "    use_mf_collat=True, mf_collat_gain=0.3\n",
        ")\n",
        "snn = CerebellarNet(cfg_sn, device=None)\n",
        "\n",
        "env_sn = TiltPlateEnv(env_config={\"tau_max_x\":0.2, \"tau_max_y\":0.2})\n",
        "ret, nstep = train_one_episode_snn(env_sn, snn, steps=2000, rate_scale=60.0)\n",
        "print(f\"SNN-only episode return={ret:.3f}, steps={nstep}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO-MdsPiLn3W",
        "outputId": "72a37a47-8478-4630-d2dd-19ccc31fb9d8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SNN-only episode return=-88.428, steps=2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gputil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5D_cFmSOi3v",
        "outputId": "6f8660df-47cf-40ce-a31d-65faf56562b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.12/dist-packages (1.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray.rllib.algorithms.dqn import DQNConfig\n",
        "\n",
        "ray.shutdown(); ray.init(ignore_reinit_error=True, include_dashboard=False)\n",
        "\n",
        "cfg = (\n",
        "    DQNConfig()\n",
        "    .environment(\n",
        "        env=TiltPlateEnv,            # 클래스로!\n",
        "        env_config={\"tau_max_x\":0.2, \"tau_max_y\":0.2}\n",
        "    )\n",
        "    .framework(\"torch\")\n",
        "    .api_stack(enable_rl_module_and_learner=True)\n",
        "    .env_runners(num_env_runners=0) # Colab 단일 프로세스\n",
        ")\n",
        "\n",
        "algo = cfg.build()\n",
        "res = algo.train()\n",
        "print({\n",
        "    \"len_mean\": res.get(\"env_runners\", {}).get(\"episode_len_mean\"),\n",
        "    \"ret_mean\": res.get(\"env_runners\", {}).get(\"episode_return_mean\"),\n",
        "})\n",
        "algo.stop(); ray.shutdown()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvg_hDuPMmFu",
        "outputId": "b25747cf-793c-47de-ba09-cfa8c57a7237"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-22 02:36:24,342\tINFO worker.py:1951 -- Started a local Ray instance.\n",
            "/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py:644: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'len_mean': None, 'ret_mean': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 0) Imports & constants ====\n",
        "import math, numpy as np\n",
        "import mujoco as mj\n",
        "from gymnasium import Env, spaces\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "DT = 1e-4   # 0.1 ms\n",
        "V  = 50.0   # m/s\n",
        "PLATE_HALF = 0.15\n",
        "SITES_N = 10\n",
        "BALL_R  = 0.015\n",
        "BALL_M  = 0.03\n",
        "\n",
        "# ==== 1) Delay + grid ====\n",
        "class DelayLine:\n",
        "    def __init__(self, max_steps, n_channels):\n",
        "        self.buf = np.zeros((max_steps+1, n_channels), dtype=np.float32)\n",
        "        self.ptr = 0\n",
        "        self.max_steps = max_steps\n",
        "        self.n = n_channels\n",
        "    def push(self, x_t):\n",
        "        self.buf[self.ptr, :] = x_t\n",
        "        self.ptr = (self.ptr + 1) % (self.max_steps+1)\n",
        "    def read_delayed(self, ks):\n",
        "        idx = (self.ptr - 1 - ks) % (self.max_steps+1)\n",
        "        return self.buf[idx, np.arange(self.n)]\n",
        "\n",
        "def manhattan_delay_steps(x, y, dt=DT, v=V):\n",
        "    return int(np.ceil((abs(x)+abs(y)) / (v*dt)))\n",
        "\n",
        "def make_site_grid(n=SITES_N, half=PLATE_HALF, z=0.006):\n",
        "    xs = np.linspace(-half, half, n); ys = np.linspace(-half, half, n)\n",
        "    return [(x, y, z, f\"sn_i{i}_j{j}\") for i,y in enumerate(ys) for j,x in enumerate(xs)]\n",
        "\n",
        "def site_xml_lines(pts):\n",
        "    return \"\\n      \".join(\n",
        "        f'<site name=\"{name}\" pos=\"{x:.4f} {y:.4f} {z:.4f}\" size=\"0.002\" type=\"sphere\" rgba=\"0.2 0.8 0.2 0.5\"/>'\n",
        "        for (x,y,z,name) in pts\n",
        "    )\n",
        "\n",
        "# 4-edge -> 2-hinge torque\n",
        "def edge4_to_torque2(u4, w=(1.,1.,1.,1.), max_tau=(0.2,0.2)):\n",
        "    uU, uR, uD, uL = u4; wU, wR, wD, wL = w\n",
        "    tau_y = (wU*uU - wD*uD) * max_tau[1]\n",
        "    tau_x = (wR*uR - wL*uL) * max_tau[0]\n",
        "    return float(np.clip(tau_x, -max_tau[0], max_tau[0])), float(np.clip(tau_y, -max_tau[1], max_tau[1]))\n",
        "\n",
        "# ==== 2) MJCF XML ====\n",
        "sites = make_site_grid()\n",
        "xml = f\"\"\"\n",
        "<mujoco model=\"tilt_plate\">\n",
        "  <compiler angle=\"degree\" inertiafromgeom=\"true\"/>\n",
        "  <option timestep=\"{DT:.7f}\" gravity=\"0 0 -9.81\" integrator=\"RK4\"/>\n",
        "  <default>\n",
        "    <geom  condim=\"6\" margin=\"0.001\" solimp=\"0.9 0.95 0.001\" solref=\"0.002 1\"/>\n",
        "    <default class=\"plate\">\n",
        "      <geom type=\"box\" friction=\"0.8 0.003 0.001\" rgba=\"0.8 0.8 0.85 1\"/>\n",
        "    </default>\n",
        "    <default class=\"ball\">\n",
        "      <geom type=\"sphere\" friction=\"0.9 0.005 0.002\" rgba=\"0.9 0.3 0.3 1\"/>\n",
        "    </default>\n",
        "    <joint armature=\"0.002\" damping=\"0.1\" limited=\"true\"/>\n",
        "    <motor gear=\"1.0\" ctrllimited=\"true\" ctrlrange=\"-1.0 1.0\"/>\n",
        "  </default>\n",
        "  <worldbody>\n",
        "    <body name=\"plate_base\" pos=\"0 0 0\">\n",
        "      <joint name=\"hinge_x\" type=\"hinge\" axis=\"1 0 0\" range=\"-5 5\"/>\n",
        "      <joint name=\"hinge_y\" type=\"hinge\" axis=\"0 1 0\" range=\"-5 5\"/>\n",
        "      <geom name=\"plate_geom\" class=\"plate\" size=\"{PLATE_HALF} {PLATE_HALF} 0.005\" mass=\"1.0\"/>\n",
        "      {site_xml_lines(sites)}\n",
        "    </body>\n",
        "    <body name=\"ball\" pos=\"0 0 {BALL_R+0.01:.4f}\">\n",
        "      <freejoint name=\"ball_free\"/>\n",
        "      <geom name=\"ball_geom\" class=\"ball\" size=\"{BALL_R}\" mass=\"{BALL_M}\"/>\n",
        "    </body>\n",
        "  </worldbody>\n",
        "  <actuator>\n",
        "    <motor name=\"mx\" joint=\"hinge_x\" gear=\"1\"/>\n",
        "    <motor name=\"my\" joint=\"hinge_y\" gear=\"1\"/>\n",
        "  </actuator>\n",
        "</mujoco>\n",
        "\"\"\".strip()\n",
        "\n",
        "# ==== 3) Discrete actions {0,1}^4 ====\n",
        "N_BINS = 2\n",
        "BINS   = np.array([0.0, 1.0], dtype=np.float32)\n",
        "def idx_to_u4(idx, n_bins=N_BINS, bins=BINS):\n",
        "    u = np.empty(4, dtype=np.float32)\n",
        "    for k in range(4):\n",
        "        u[k] = bins[idx % n_bins]; idx //= n_bins\n",
        "    return u  # [U,R,D,L]\n",
        "\n",
        "@dataclass\n",
        "class PlateConfig:\n",
        "    dt: float = DT\n",
        "    substeps: int = 1\n",
        "    max_steps: int = 3000\n",
        "    tau_max_x: float = 0.2\n",
        "    tau_max_y: float = 0.2\n",
        "\n",
        "# ==== 4) SNN 유틸 ====\n",
        "def encode_mf_spikes(obs, rate_scale=60.0, dt=DT):\n",
        "    x = np.asarray(obs, dtype=np.float32)\n",
        "    rng = float(np.ptp(x))\n",
        "    x = (x - float(x.min())) / (rng + 1e-6) if rng >= 1e-12 else np.zeros_like(x, dtype=np.float32)\n",
        "    lam = x * rate_scale * dt\n",
        "    return (np.random.rand(x.shape[0]) < lam).astype(np.float32)\n",
        "\n",
        "def cf_from_transition(prev_r, curr_r, k_pos=+1.0, k_neg=-1.0):\n",
        "    dr = curr_r - prev_r\n",
        "    return k_pos if dr < 0 else (k_neg if dr > 0 else 0.0)\n",
        "\n",
        "# ==== 5) Env with internal CerebellarNet (SNN 임베딩 반환) ====\n",
        "class TiltPlateSNNDQNEnv(Env):\n",
        "    metadata = {\"render_modes\": []}\n",
        "    def __init__(self, env_config=None):\n",
        "        # EnvContext -> dict (RLlib 호환)\n",
        "        try:\n",
        "            from ray.rllib.env.env_context import EnvContext\n",
        "            if isinstance(env_config, EnvContext):\n",
        "                env_config = dict(env_config)\n",
        "        except Exception:\n",
        "            pass\n",
        "        env_config = env_config or {}\n",
        "        self.cfg = PlateConfig(\n",
        "            dt       = env_config.get(\"dt\", DT),\n",
        "            substeps = env_config.get(\"substeps\", 1),\n",
        "            max_steps= env_config.get(\"max_steps\", 3000),\n",
        "            tau_max_x= env_config.get(\"tau_max_x\", 0.2),\n",
        "            tau_max_y= env_config.get(\"tau_max_y\", 0.2),\n",
        "        )\n",
        "\n",
        "        # MuJoCo\n",
        "        self.model = mj.MjModel.from_xml_string(xml)\n",
        "        self.data  = mj.MjData(self.model)\n",
        "        self.site_xy = [(x,y) for (x,y,_,_) in sites]\n",
        "        self.ks = np.array([manhattan_delay_steps(x,y) for (x,y) in self.site_xy], dtype=np.int32)\n",
        "        self.delay = DelayLine(int(self.ks.max()), len(self.site_xy))\n",
        "\n",
        "        # --- CerebellarNet 내부 생성 ---\n",
        "        # SimulationConfig는 네가 정의한 dataclass 사용\n",
        "        self.sn_cfg = SimulationConfig(\n",
        "            n_mf=108, n_grc=512, n_pkj=8, n_motor=4,\n",
        "            tau_pre_ms=5.0, tau_post_ms=5.0, tau_e_ms=50.0,\n",
        "            dt_ms=0.1, eta=1e-3, max_dw_per_step=1e-3, scale_every=100,\n",
        "            use_mf_collat=True, mf_collat_gain=0.3\n",
        "        )\n",
        "        self.snn = CerebellarNet(self.sn_cfg)\n",
        "        self.prev_r = None\n",
        "\n",
        "        # 관측을 SNN 임베딩 z로 축소 (여기선 간단히 motor 스파이크 4개 + PKJ 스파이크 8개 = 12차원)\n",
        "        self.z_dim = 12\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.z_dim,), dtype=np.float32)\n",
        "\n",
        "        # 액션은 DQN용 이산 16개\n",
        "        self.action_space = spaces.Discrete(N_BINS**4)\n",
        "        self.step_count = 0\n",
        "        self._reset_ball()\n",
        "\n",
        "    # ---- 내부: raw obs 생성 (기존과 동일) ----\n",
        "    def _obs_raw(self):\n",
        "        bx, by = self.data.qpos[0], self.data.qpos[1]\n",
        "        vx, vy = self.data.qvel[0], self.data.qvel[1]\n",
        "        jx = mj.mj_name2id(self.model, mj.mjtObj.mjOBJ_JOINT, \"hinge_x\")\n",
        "        jy = mj.mj_name2id(self.model, mj.mjtObj.mjOBJ_JOINT, \"hinge_y\")\n",
        "        hx = self.data.qpos[self.model.jnt_qposadr[jx]]\n",
        "        hy = self.data.qpos[self.model.jnt_qposadr[jy]]\n",
        "        hvx = self.data.qvel[self.model.jnt_dofadr[jx]]\n",
        "        hvy = self.data.qvel[self.model.jnt_dofadr[jy]]\n",
        "        delayed = self.delay.read_delayed(self.ks)\n",
        "        return np.concatenate([[bx,by,vx,vy,hx,hy,hvx,hvy], delayed]).astype(np.float32)\n",
        "\n",
        "    # ---- SNN 임베딩: obs_raw -> spikes -> forward -> z ----\n",
        "    def _embed_with_snn(self, obs_raw):\n",
        "        mf_spk = encode_mf_spikes(obs_raw, rate_scale=60.0, dt=DT)\n",
        "        mo_spk = self.snn.forward(mf_spk, cf_spikes=None)    # (4,) binary\n",
        "        # PKJ 스파이크는 캐시에 있음\n",
        "        pkj_spk = self.snn._last['pkj']\n",
        "        z = np.zeros(self.z_dim, dtype=np.float32)\n",
        "        z[:4] = mo_spk\n",
        "        z[4:12] = pkj_spk[:8] if pkj_spk.shape[0] >= 8 else np.pad(pkj_spk, (0, 8-pkj_spk.shape[0]))\n",
        "        return z, mo_spk  # z: 관측, mo_spk: 내부 행동 생성용\n",
        "\n",
        "    def _reset_ball(self):\n",
        "        self.data.qpos[:] = 0.0; self.data.qvel[:] = 0.0\n",
        "        self.data.qpos[0] = np.random.uniform(-0.03, 0.03)\n",
        "        self.data.qpos[1] = np.random.uniform(-0.03, 0.03)\n",
        "        self.data.qpos[2] = BALL_R + 0.005\n",
        "        self.delay = DelayLine(int(self.ks.max()), len(self.site_xy))\n",
        "        self.snn.reset()\n",
        "        bx, by = self.data.qpos[0], self.data.qpos[1]\n",
        "        self.prev_r = float(math.hypot(bx, by))\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self._reset_ball()\n",
        "        self.step_count = 0\n",
        "        self.delay.push(np.zeros(len(self.site_xy), dtype=np.float32))\n",
        "        obs_raw = self._obs_raw()\n",
        "        z, _ = self._embed_with_snn(obs_raw)  # 초기 z\n",
        "        return z, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # 1) SNN이 제안한 모터 스파이크 기반 u4를 기본으로 하되,\n",
        "        #    DQN의 action(=16-way)을 \"게이트\"로 사용: action이 0이면 SNN u4 그대로,\n",
        "        #    그 외에는 idx_to_u4(action)로 override (간단한 결합 전략; 필요 시 바꿔도 됨)\n",
        "        obs_raw = self._obs_raw()\n",
        "        z, mo_spk = self._embed_with_snn(obs_raw)\n",
        "        u4_snn = mo_spk.astype(np.float32)\n",
        "        u4_dqn = idx_to_u4(int(action))\n",
        "        u4 = u4_snn if int(action) == 0 else u4_dqn  # policy-mix 예시\n",
        "\n",
        "        tau_x, tau_y = edge4_to_torque2(u4, max_tau=(self.cfg.tau_max_x, self.cfg.tau_max_y))\n",
        "        self.data.ctrl[0] = tau_x; self.data.ctrl[1] = tau_y\n",
        "\n",
        "        # 센서 push + 물리 스텝\n",
        "        self.delay.push(self._sense_now())\n",
        "        for _ in range(self.cfg.substeps):\n",
        "            mj.mj_step(self.model, self.data)\n",
        "\n",
        "        # 보상/종료\n",
        "        bx, by = self.data.qpos[0], self.data.qpos[1]\n",
        "        vx, vy = self.data.qvel[0], self.data.qvel[1]\n",
        "        r = math.hypot(bx, by)\n",
        "        speed = math.hypot(vx, vy)\n",
        "        reward = -r - 0.1*speed - 0.001*float(np.sum(u4*u4))\n",
        "\n",
        "        # CF 학습\n",
        "        cf = cf_from_transition(self.prev_r, r, k_pos=+1.0, k_neg=-1.0)\n",
        "        self.snn.learn(cf_signal=cf)\n",
        "        self.prev_r = r\n",
        "\n",
        "        # 다음 관측(z) 업데이트\n",
        "        obs_raw2 = self._obs_raw()\n",
        "        z_next, _ = self._embed_with_snn(obs_raw2)\n",
        "\n",
        "        self.step_count += 1\n",
        "        terminated = self.step_count >= self.cfg.max_steps\n",
        "        truncated = False\n",
        "        return z_next, reward, terminated, truncated, {}\n",
        "\n",
        "    def _sense_now(self):\n",
        "        bx, by = self.data.qpos[0], self.data.qpos[1]\n",
        "        return np.array([math.exp(- (abs(bx-x)+abs(by-y))/0.05) for (x,y) in self.site_xy], dtype=np.float32)\n",
        "\n",
        "# ==== 6) 빠른 sanity check ====\n",
        "env = TiltPlateSNNDQNEnv()\n",
        "z, _ = env.reset(seed=42)\n",
        "print(\"z_dim:\", z.shape)\n",
        "for _ in range(5):\n",
        "    z, r, term, trunc, _ = env.step(0)  # action=0이면 SNN u4 사용\n",
        "print(\"step OK; last reward:\", r)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNgK98oCOlwV",
        "outputId": "d55fb856-fbeb-4c1c-e513-dde5991ea974"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z_dim: (12,)\n",
            "step OK; last reward: -0.04558527545711828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray.rllib.algorithms.dqn import DQNConfig\n",
        "\n",
        "ray.shutdown(); ray.init(ignore_reinit_error=True, include_dashboard=False)\n",
        "\n",
        "cfg = (\n",
        "    DQNConfig()\n",
        "    .environment(\n",
        "        env=TiltPlateSNNDQNEnv,           # ← 클래스로!\n",
        "        env_config={\"tau_max_x\":0.2, \"tau_max_y\":0.2}\n",
        "    )\n",
        "    .framework(\"torch\")\n",
        "    .api_stack(enable_rl_module_and_learner=True)\n",
        "    .env_runners(num_env_runners=4, num_gpus_per_env_runner=0.25)      # ← rollouts(...) 대체\n",
        ")\n",
        "\n",
        "algo = cfg.build()\n",
        "res = algo.train()\n",
        "print({\n",
        "    \"len_mean\": res.get(\"env_runners\", {}).get(\"episode_len_mean\"),\n",
        "    \"ret_mean\": res.get(\"env_runners\", {}).get(\"episode_return_mean\"),\n",
        "})\n",
        "algo.stop(); ray.shutdown()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy3CjXnKPYu7",
        "outputId": "c9543c66-c381-4268-ed03-3c2bab97fec4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-22 02:36:56,004\tINFO worker.py:1951 -- Started a local Ray instance.\n",
            "/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py:644: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'len_mean': None, 'ret_mean': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === B. RLlib DQN 학습 ===\n",
        "# 전제: TiltPlateSNNDQNEnv 가 정의되어 있음 (SNN 내장, 관측 z 반환, Discrete(16) 액션)\n",
        "!pip -q install -U \"ray[rllib]==2.49.2\"\n",
        "\n",
        "import ray\n",
        "from ray.rllib.algorithms.dqn import DQNConfig\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init(ignore_reinit_error=True, include_dashboard=False)\n",
        "\n",
        "# 환경을 \"클래스\"로 넘기고 env_runners 사용 (최신 API)\n",
        "cfg = (\n",
        "    DQNConfig()\n",
        "    .environment(\n",
        "        env=TiltPlateSNNDQNEnv,                 # 클래스로 전달 (인스턴스 금지)  :contentReference[oaicite:4]{index=4}\n",
        "        env_config={\"tau_max_x\":0.2, \"tau_max_y\":0.2}\n",
        "    )\n",
        "    .framework(\"torch\")\n",
        "    .api_stack(enable_rl_module_and_learner=True)\n",
        "    .env_runners(\n",
        "        num_env_runners=0,                      # Colab 단일 프로세스\n",
        "        # rollout_fragment_length 등 필요시 여기서 조정  :contentReference[oaicite:5]{index=5}\n",
        "    )\n",
        "    # (옵션) 학습 하이퍼파라미터 일부\n",
        "    .training(\n",
        "        gamma=0.99,\n",
        "        lr=1e-3,\n",
        "        train_batch_size=512,\n",
        "        replay_buffer_config={\"capacity\": 50000},\n",
        "        n_step=1,\n",
        "        target_network_update_freq=500,\n",
        "    )\n",
        ")\n",
        "\n",
        "algo = cfg.build()\n",
        "\n",
        "# 짧게 몇 iteration만 데모 학습\n",
        "for i in range(3):\n",
        "    res = algo.train()\n",
        "    print(f\"[Iter {i+1}] len_mean={res.get('env_runners', {}).get('episode_len_mean')}, \"\n",
        "          f\"ret_mean={res.get('env_runners', {}).get('episode_return_mean')}\")\n",
        "\n",
        "# 체크포인트 저장\n",
        "ckpt = algo.save()\n",
        "print(\"Saved checkpoint:\", ckpt)\n",
        "algo.stop()\n",
        "ray.shutdown()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "2_jl5fHgP6Zo",
        "outputId": "57c3b600-58f3-4a23-c856-7f1c421315a7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-22 02:37:31,472\tINFO worker.py:1951 -- Started a local Ray instance.\n",
            "/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py:644: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 1] len_mean=None, ret_mean=None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1030056382.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# 짧게 몇 iteration만 데모 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     print(f\"[Iter {i+1}] len_mean={res.get('env_runners', {}).get('episode_len_mean')}, \"\n\u001b[1;32m     41\u001b[0m           f\"ret_mean={res.get('env_runners', {}).get('episode_return_mean')}\")\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/tune/trainable/trainable.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mskipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskip_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/rllib/algorithms/algorithm.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_env_runner_and_connector_v2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m                 \u001b[0mtrain_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_one_training_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/rllib/algorithms/algorithm.py\u001b[0m in \u001b[0;36m_run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3394\u001b[0m                     \u001b[0;31m# Try to train one step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3395\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTIMERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAINING_STEP_TIMER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3396\u001b[0;31m                         \u001b[0mtraining_step_return_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3397\u001b[0m                         \u001b[0mhas_run_once\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/rllib/algorithms/dqn/dqn.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# New API stack (RLModule, Learner, EnvRunner, ConnectorV2).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_step_new_api_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_training_step_new_api_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/rllib/algorithms/dqn/dqn.py\u001b[0m in \u001b[0;36m_training_step_new_api_stack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m                 \u001b[0;31m# Perform an update on the buffer-sampled train batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTIMERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNER_UPDATE_TIMER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m                     learner_results = self.learner_group.update(\n\u001b[0m\u001b[1;32m    715\u001b[0m                         \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m                         timesteps={\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/rllib/core/learner/learner_group.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, batch, batches, batch_refs, episodes, episodes_refs, data_iterators, training_data, timesteps, async_update, return_state, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Return the single Learner's update results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             return [\n\u001b[0;32m--> 318\u001b[0;31m                 self._learner.update(\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mtimesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/rllib/core/learner/learner.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, batch, batches, batch_refs, episodes, episodes_refs, data_iterators, training_data, timesteps, num_total_minibatches, num_epochs, minibatch_size, shuffle_batch_per_epoch, _no_metrics_reduce, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0;31m# Make the actual in-graph/traced `_update` call. This should return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m             \u001b[0;31m# all tensor values (no numpy).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m             \u001b[0mfwd_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_per_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_minibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0;31m# TODO (sven): Maybe move this into loop above to get metrics more accuratcely\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/rllib/core/learner/torch/torch_learner.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uncompiled_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_possibly_compiled_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mOverrideToImplementCustomLogic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/rllib/core/learner/torch/torch_learner.py\u001b[0m in \u001b[0;36m_uncompiled_update\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mfwd_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mloss_per_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfwd_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfwd_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_per_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/rllib/core/learner/learner.py\u001b[0m in \u001b[0;36mcompute_losses\u001b[0;34m(self, fwd_out, batch)\u001b[0m\n\u001b[1;32m    933\u001b[0m                 )\n\u001b[1;32m    934\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                 loss = self.compute_loss_for_module(\n\u001b[0m\u001b[1;32m    936\u001b[0m                     \u001b[0mmodule_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_for_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/rllib/algorithms/dqn/torch/dqn_torch_learner.py\u001b[0m in \u001b[0;36mcompute_loss_for_module\u001b[0;34m(self, module_id, config, batch, fwd_out)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# space - we might need the one_hot_selection. Also test performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         q_selected = torch.nan_to_num(\n\u001b[0;32m---> 93\u001b[0;31m             torch.gather(\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mq_curr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Visible GPUs:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"all\"))\n"
      ],
      "metadata": {
        "id": "_FbWLEnbXtbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ray.rllib.algorithms.dqn import DQNConfig\n",
        "\n",
        "cfg = (\n",
        "    DQNConfig()\n",
        "    .environment(env=TiltPlateSNNDQNEnv,\n",
        "    env_config={\"tau_max_x\":0.2, \"tau_max_y\":0.2, \"max_steps\":500})\n",
        "    .framework(\"torch\")\n",
        "    .api_stack(enable_rl_module_and_learner=True)\n",
        "    .resources(num_gpus=1)  # <- 핵심\n",
        "    .env_runners(\n",
        "        num_env_runners=0,                 # Colab 단일 프로세스\n",
        "        rollout_fragment_length=2000,      # ← 조각 길이 상향\n",
        "    )\n",
        "    .training(\n",
        "        train_batch_size=2000,             # 조각 합쳐 도달\n",
        "    )\n",
        "    .evaluation(\n",
        "        evaluation_interval=1,             # ← 매 iter 평가\n",
        "        evaluation_duration=3,             # 평가 에피소드 수\n",
        "        evaluation_duration_unit=\"episodes\"\n",
        "    )\n",
        ")\n",
        "print(\"done config\")\n",
        "algo = cfg.build()\n",
        "for i in range(100):\n",
        "    res = algo.train()\n",
        "    print(f\"[Iter {i+1}] len_mean={res.get('env_runners', {}).get('episode_len_mean')}, \"\n",
        "          f\"ret_mean={res.get('env_runners', {}).get('episode_return_mean')}\")\n",
        "\n",
        "    print(\"eval :\", res.get(\"evaluation\",{}).get(\"env_runners\",{}).get(\"episode_len_mean\"),\n",
        "                res.get(\"evaluation\",{}).get(\"env_runners\",{}).get(\"episode_return_mean\"))\n",
        "# 체크포인트 저장\n",
        "ckpt = algo.save()\n",
        "print(\"Saved checkpoint:\", ckpt)\n",
        "algo.stop()\n",
        "ray.shutdown()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IhZMLr_P7CE",
        "outputId": "dfbe3eba-39da-48b6-9888-425d2832bc04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done config\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py:644: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
            "/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py:644: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 1] len_mean=1000.0, ret_mean=-54.680862076513634\n",
            "eval : 1000.0 -133.98082907302958\n",
            "[Iter 2] len_mean=1000.0, ret_mean=-59.28181991454788\n",
            "eval : 1000.0 -88.80728995569656\n",
            "[Iter 3] len_mean=1000.0, ret_mean=-55.02706563014721\n",
            "eval : 1000.0 -115.33785168735693\n",
            "[Iter 4] len_mean=1000.0, ret_mean=-64.15851102717451\n",
            "eval : 1000.0 -125.59594962859923\n",
            "[Iter 5] len_mean=1000.0, ret_mean=-77.50992528969081\n",
            "eval : 1000.0 -124.0207069650441\n",
            "[Iter 6] len_mean=1000.0, ret_mean=-83.34871428214757\n",
            "eval : 1000.0 -129.64167613052365\n",
            "[Iter 7] len_mean=1000.0, ret_mean=-91.4293717612594\n",
            "eval : 1000.0 -135.68638617273302\n",
            "[Iter 8] len_mean=1000.0, ret_mean=-98.05232661974252\n",
            "eval : 1000.0 -134.4524216608119\n",
            "[Iter 9] len_mean=1000.0, ret_mean=-100.57023904651383\n",
            "eval : 1000.0 -132.40050855945287\n",
            "[Iter 10] len_mean=1000.0, ret_mean=-101.85493819882281\n",
            "eval : 1000.0 -130.72013573421626\n",
            "[Iter 11] len_mean=1000.0, ret_mean=-102.57004056947343\n",
            "eval : 1000.0 -130.3519952872602\n",
            "[Iter 12] len_mean=1000.0, ret_mean=-103.87648733208128\n",
            "eval : 1000.0 -129.84904116645384\n",
            "[Iter 13] len_mean=1000.0, ret_mean=-104.3301259012068\n",
            "eval : 1000.0 -129.2395375506256\n",
            "[Iter 14] len_mean=1000.0, ret_mean=-105.46060399065426\n",
            "eval : 1000.0 -128.13483785698523\n",
            "[Iter 15] len_mean=1000.0, ret_mean=-106.38045037736615\n",
            "eval : 1000.0 -122.71844697734343\n",
            "[Iter 16] len_mean=1000.0, ret_mean=-102.80945954906812\n",
            "eval : 1000.0 -117.7929297558225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nif6l78zYbGv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}